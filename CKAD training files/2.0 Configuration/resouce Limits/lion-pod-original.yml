apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: 2019-05-21T10:01:42Z
  name: lion
  namespace: default
  resourceVersion: "791"
  selfLink: /api/v1/namespaces/default/pods/lion
  uid: 6efd37ef-7baf-11e9-8f80-0242ac110017
spec:
  containers:
  - args:
    - -cpus
    - "1"
    image: vish/stress
    imagePullPolicy: Always
    name: cpu-stress
    resources:
      limits:
        cpu: "200"
      requests:
        cpu: "100"
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-fht5m
      readOnly: true
  dnsPolicy: ClusterFirst
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-fht5m
    secret:
      defaultMode: 420
      secretName: default-token-fht5m
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: 2019-05-21T10:01:42Z
    message: '0/2 nodes are available: 2 Insufficient cpu.'
    reason: Unschedulable
    status: "False"
    type: PodScheduled
  phase: Pending
  qosClass: Burstable